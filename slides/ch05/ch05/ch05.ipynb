{"nbformat":4,"nbformat_minor":0,"metadata":{"anaconda-cloud":{},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.1"},"toc":{"nav_menu":{},"number_sections":true,"sideBar":true,"skip_h1_title":false,"title_cell":"Table of Contents","title_sidebar":"Contents","toc_cell":false,"toc_position":{},"toc_section_display":true,"toc_window_display":false},"colab":{"name":"ch05.ipynb","provenance":[]}},"cells":[{"cell_type":"markdown","metadata":{"id":"deYhiYyK_-q8","colab_type":"text"},"source":["*Python Machine Learning 3rd Edition* by [Sebastian Raschka](https://sebastianraschka.com), Packt Publishing Ltd. 2019\n","\n","Code Repository: https://github.com/rasbt/python-machine-learning-book-3rd-edition\n","\n","Code License: [MIT License](https://github.com/rasbt/python-machine-learning-book-3rd-edition/blob/master/LICENSE.txt)"]},{"cell_type":"markdown","metadata":{"id":"1nn63KhY_-rB","colab_type":"text"},"source":["# Python Machine Learning - Code Examples"]},{"cell_type":"markdown","metadata":{"id":"xF0XHYkl_-rD","colab_type":"text"},"source":["# Chapter 5 - Compressing Data via Dimensionality Reduction"]},{"cell_type":"code","metadata":{"id":"P8vc7hVQ_-rH","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"wtgRRr3L_-rS","colab_type":"text"},"source":["<br>\n","<br>"]},{"cell_type":"markdown","metadata":{"id":"-CcIJGon_-rT","colab_type":"text"},"source":["### Overview"]},{"cell_type":"markdown","metadata":{"id":"qj3PtaxD_-rV","colab_type":"text"},"source":["- [Unsupervised dimensionality reduction via principal component analysis 128](#Unsupervised-dimensionality-reduction-via-principal-component-analysis-128)\n","  - [The main steps behind principal component analysis](#The-main-steps-behind-principal-component-analysis)\n","  - [Extracting the principal components step-by-step](#Extracting-the-principal-components-step-by-step)\n","  - [Total and explained variance](#Total-and-explained-variance)\n","  - [Feature transformation](#Feature-transformation)\n","  - [Principal component analysis in scikit-learn](#Principal-component-analysis-in-scikit-learn)\n","- [Supervised data compression via linear discriminant analysis](#Supervised-data-compression-via-linear-discriminant-analysis)\n","  - [Principal component analysis versus linear discriminant analysis](#Principal-component-analysis-versus-linear-discriminant-analysis)\n","  - [The inner workings of linear discriminant analysis](#The-inner-workings-of-linear-discriminant-analysis)\n","  - [Computing the scatter matrices](#Computing-the-scatter-matrices)\n","  - [Selecting linear discriminants for the new feature subspace](#Selecting-linear-discriminants-for-the-new-feature-subspace)\n","  - [Projecting examples onto the new feature space](#Projecting-examples-onto-the-new-feature-space)\n","  - [LDA via scikit-learn](#LDA-via-scikit-learn)\n","- [Using kernel principal component analysis for nonlinear mappings](#Using-kernel-principal-component-analysis-for-nonlinear-mappings)\n","  - [Kernel functions and the kernel trick](#Kernel-functions-and-the-kernel-trick)\n","  - [Implementing a kernel principal component analysis in Python](#Implementing-a-kernel-principal-component-analysis-in-Python)\n","    - [Example 1 – separating half-moon shapes](#Example-1:-Separating-half-moon-shapes)\n","    - [Example 2 – separating concentric circles](#Example-2:-Separating-concentric-circles)\n","  - [Projecting new data points](#Projecting-new-data-points)\n","  - [Kernel principal component analysis in scikit-learn](#Kernel-principal-component-analysis-in-scikit-learn)\n","- [Summary](#Summary)"]},{"cell_type":"markdown","metadata":{"id":"5Fy5sf9T_-rX","colab_type":"text"},"source":["<br>\n","<br>"]},{"cell_type":"code","metadata":{"id":"4frKGZBq_-rZ","colab_type":"code","colab":{}},"source":["from IPython.display import Image\n","%matplotlib inline\n","import numpy as np"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"JJgBs1ZKkVLs","colab_type":"code","outputId":"c308bf63-cbe9-4c90-e113-4af9667aaa5a","executionInfo":{"status":"error","timestamp":1584096448262,"user_tz":-180,"elapsed":1926,"user":{"displayName":"Savas Yıldırım","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhdhYZMfq-hvK2xI7HqkzvJuCbfgFrIs4wypQEm5w=s64","userId":"10717726124681851716"}},"colab":{"base_uri":"https://localhost:8080/","height":214}},"source":["import os\n","from google.colab import drive\n","drive.mount('/content/drive')\n","os.chdir(\"drive/My Drive/Courses/cmpe 343/ch05\")"],"execution_count":94,"outputs":[{"output_type":"stream","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"],"name":"stdout"},{"output_type":"error","ename":"FileNotFoundError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-94-ac6b3d2fd495>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mgoogle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolab\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdrive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mdrive\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/drive'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"drive/My Drive/Courses/cmpe 343/ch05\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'drive/My Drive/Courses/cmpe 343/ch05'"]}]},{"cell_type":"markdown","metadata":{"id":"vXe33k3J_-re","colab_type":"text"},"source":["# Unsupervised dimensionality reduction via principal component analysis"]},{"cell_type":"markdown","metadata":{"id":"kmaIxw_N_-rg","colab_type":"text"},"source":["## The main steps behind principal component analysis"]},{"cell_type":"code","metadata":{"id":"CwlUug9t_-ri","colab_type":"code","colab":{}},"source":["Image(filename='images/05_01.png', width=400) "],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"1VeyfAJ-_-ro","colab_type":"text"},"source":["## Extracting the principal components step-by-step"]},{"cell_type":"code","metadata":{"id":"QMIh72wU_-rq","colab_type":"code","colab":{}},"source":["import pandas as pd\n","\n","df_wine = pd.read_csv('https://archive.ics.uci.edu/ml/'\n","                      'machine-learning-databases/wine/wine.data',\n","                      header=None)\n","\n","# if the Wine dataset is temporarily unavailable from the\n","# UCI machine learning repository, un-comment the following line\n","# of code to load the dataset from a local path:\n","\n","# df_wine = pd.read_csv('wine.data', header=None)\n","\n","df_wine.columns = ['Class label', 'Alcohol', 'Malic acid', 'Ash',\n","                   'Alcalinity of ash', 'Magnesium', 'Total phenols',\n","                   'Flavanoids', 'Nonflavanoid phenols', 'Proanthocyanins',\n","                   'Color intensity', 'Hue',\n","                   'OD280/OD315 of diluted wines', 'Proline']\n","\n","df_wine.head()"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"PTSRnw_I_-rv","colab_type":"text"},"source":["<hr>"]},{"cell_type":"markdown","metadata":{"id":"Ov1DBQxO_-rw","colab_type":"text"},"source":["Splitting the data into 70% training and 30% test subsets."]},{"cell_type":"code","metadata":{"id":"Q0NSpsBD_-ry","colab_type":"code","colab":{}},"source":["from sklearn.model_selection import train_test_split\n","\n","X, y = df_wine.iloc[:, 1:].values, df_wine.iloc[:, 0].values\n","\n","X_train, X_test, y_train, y_test = \\\n","    train_test_split(X, y, test_size=0.3, \n","                     stratify=y,\n","                     random_state=0)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"34nZtYDl_-r4","colab_type":"text"},"source":["Standardizing the data."]},{"cell_type":"code","metadata":{"id":"SugFgyJS_-r6","colab_type":"code","colab":{}},"source":["from sklearn.preprocessing import StandardScaler\n","\n","sc = StandardScaler()\n","X_train_std = sc.fit_transform(X_train)\n","X_test_std = sc.transform(X_test)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"fBf7TIn0lLIh","colab_type":"code","colab":{}},"source":["X_test[:4,:4]"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"IapAPHjvlErf","colab_type":"code","colab":{}},"source":["np.round(X_test_std[:4,:4],2)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"RoSPRtIf_-sB","colab_type":"text"},"source":["Eigendecomposition of the covariance matrix."]},{"cell_type":"code","metadata":{"id":"7lq2QXaR_-sD","colab_type":"code","colab":{}},"source":["import numpy as np\n","cov_mat = np.cov(X_train_std.T)\n","eigen_vals, eigen_vecs = np.linalg.eig(cov_mat)\n","\n","print('\\nEigenvalues \\n%s' % np.round(eigen_vals,2))"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"Rbs-xtGcl2gn","colab_type":"code","colab":{}},"source":["X_train_std.shape, eigen_vals.shape, eigen_vecs.shape"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"Yuwv9EiYmB-2","colab_type":"code","colab":{}},"source":["np.round(eigen_vecs,2)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"3bboMtld_-sI","colab_type":"text"},"source":["**Note**: \n","\n","Above, I used the [`numpy.linalg.eig`](http://docs.scipy.org/doc/numpy/reference/generated/numpy.linalg.eig.html) function to decompose the symmetric covariance matrix into its eigenvalues and eigenvectors.\n","    <pre>>>> eigen_vals, eigen_vecs = np.linalg.eig(cov_mat)</pre>\n","    This is not really a \"mistake,\" but probably suboptimal. It would be better to use [`numpy.linalg.eigh`](http://docs.scipy.org/doc/numpy/reference/generated/numpy.linalg.eigh.html) in such cases, which has been designed for [Hermetian matrices](https://en.wikipedia.org/wiki/Hermitian_matrix). The latter always returns real  eigenvalues; whereas the numerically less stable `np.linalg.eig` can decompose nonsymmetric square matrices, you may find that it returns complex eigenvalues in certain cases. (S.R.)\n"]},{"cell_type":"markdown","metadata":{"id":"pcaIgWij_-sJ","colab_type":"text"},"source":["<br>\n","<br>"]},{"cell_type":"markdown","metadata":{"id":"zZee1iWL_-sK","colab_type":"text"},"source":["## Total and explained variance"]},{"cell_type":"code","metadata":{"id":"3PP0nF4B_-sM","colab_type":"code","colab":{}},"source":["tot = sum(eigen_vals)\n","var_exp = [(i / tot) for i in sorted(eigen_vals, reverse=True)]\n","cum_var_exp = np.cumsum(var_exp)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"-aRjtwN9_-sR","colab_type":"code","colab":{}},"source":["import matplotlib.pyplot as plt\n","\n","\n","plt.bar(range(1, 14), var_exp, alpha=0.5, align='center',\n","        label='Individual explained variance')\n","plt.step(range(1, 14), cum_var_exp, where='mid',\n","         label='Cumulative explained variance')\n","plt.ylabel('Explained variance ratio')\n","plt.xlabel('Principal component index')\n","plt.legend(loc='best')\n","plt.tight_layout()\n","# plt.savefig('images/05_02.png', dpi=300)\n","plt.show()"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Mggv4cxV_-sW","colab_type":"text"},"source":["<br>\n","<br>"]},{"cell_type":"markdown","metadata":{"id":"IkrUx3Ad_-sY","colab_type":"text"},"source":["## Feature transformation"]},{"cell_type":"code","metadata":{"id":"9wtc5W3d_-sa","colab_type":"code","colab":{}},"source":["# Make a list of (eigenvalue, eigenvector) tuples\n","eigen_pairs = [(np.abs(eigen_vals[i]), eigen_vecs[:, i])\n","               for i in range(len(eigen_vals))]\n","\n","# Sort the (eigenvalue, eigenvector) tuples from high to low\n","eigen_pairs.sort(key=lambda k: k[0], reverse=True)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"pnnPqwzA_-se","colab_type":"code","colab":{}},"source":["w = np.hstack((eigen_pairs[0][1][:, np.newaxis],\n","               eigen_pairs[1][1][:, np.newaxis]))\n","print('Matrix W:\\n', w)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"N2FJc783_-si","colab_type":"text"},"source":["**Note**\n","Depending on which version of NumPy and LAPACK you are using, you may obtain the Matrix W with its signs flipped. Please note that this is not an issue: If $v$ is an eigenvector of a matrix $\\Sigma$, we have\n","\n","$$\\Sigma v = \\lambda v,$$\n","\n","where $\\lambda$ is our eigenvalue,\n","\n","\n","then $-v$ is also an eigenvector that has the same eigenvalue, since\n","$$\\Sigma \\cdot (-v) = -\\Sigma v = -\\lambda v = \\lambda \\cdot (-v).$$"]},{"cell_type":"code","metadata":{"id":"jdTPD84q_-sk","colab_type":"code","colab":{}},"source":["X_train_std[0].dot(w)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"Wg7P1NzG_-sp","colab_type":"code","colab":{}},"source":["X_train_pca = X_train_std.dot(w)\n","colors = ['r', 'b', 'g']\n","markers = ['s', 'x', 'o']\n","\n","for l, c, m in zip(np.unique(y_train), colors, markers):\n","    plt.scatter(X_train_pca[y_train == l, 0], \n","                X_train_pca[y_train == l, 1], \n","                c=c, label=l, marker=m)\n","\n","plt.xlabel('PC 1')\n","plt.ylabel('PC 2')\n","plt.legend(loc='lower left')\n","plt.tight_layout()\n","# plt.savefig('images/05_03.png', dpi=300)\n","plt.show()"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"pR45pqhf_-su","colab_type":"text"},"source":["<br>\n","<br>"]},{"cell_type":"markdown","metadata":{"id":"made0Wrk_-sw","colab_type":"text"},"source":["## Principal component analysis in scikit-learn"]},{"cell_type":"markdown","metadata":{"id":"VvJPDmF4_-sx","colab_type":"text"},"source":["**NOTE**\n","\n","The following four code cells has been added in addition to the content to the book, to illustrate how to replicate the results from our own PCA implementation in scikit-learn:"]},{"cell_type":"code","metadata":{"id":"zdt6vBrl_-sy","colab_type":"code","colab":{}},"source":["from sklearn.decomposition import PCA\n","\n","pca = PCA()\n","X_train_pca = pca.fit_transform(X_train_std)\n","pca.explained_variance_ratio_"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"1492fdFX_-s2","colab_type":"code","colab":{}},"source":["plt.bar(range(1, 14), pca.explained_variance_ratio_, alpha=0.5, align='center')\n","plt.step(range(1, 14), np.cumsum(pca.explained_variance_ratio_), where='mid')\n","plt.ylabel('Explained variance ratio')\n","plt.xlabel('Principal components')\n","\n","plt.show()"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"TKVI8tgP_-s6","colab_type":"code","colab":{}},"source":["pca = PCA(n_components=2)\n","X_train_pca = pca.fit_transform(X_train_std)\n","X_test_pca = pca.transform(X_test_std)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"xmLuydyF_-s_","colab_type":"code","colab":{}},"source":["plt.scatter(X_train_pca[:, 0], X_train_pca[:, 1])\n","plt.xlabel('PC 1')\n","plt.ylabel('PC 2')\n","plt.show()"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"rHzg6a9y_-tD","colab_type":"code","colab":{}},"source":["from matplotlib.colors import ListedColormap\n","\n","def plot_decision_regions(X, y, classifier, resolution=0.02):\n","\n","    # setup marker generator and color map\n","    markers = ('s', 'x', 'o', '^', 'v')\n","    colors = ('red', 'blue', 'lightgreen', 'gray', 'cyan')\n","    cmap = ListedColormap(colors[:len(np.unique(y))])\n","\n","    # plot the decision surface\n","    x1_min, x1_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n","    x2_min, x2_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n","    xx1, xx2 = np.meshgrid(np.arange(x1_min, x1_max, resolution),\n","                           np.arange(x2_min, x2_max, resolution))\n","    Z = classifier.predict(np.array([xx1.ravel(), xx2.ravel()]).T)\n","    Z = Z.reshape(xx1.shape)\n","    plt.contourf(xx1, xx2, Z, alpha=0.4, cmap=cmap)\n","    plt.xlim(xx1.min(), xx1.max())\n","    plt.ylim(xx2.min(), xx2.max())\n","\n","    # plot examples by class\n","    for idx, cl in enumerate(np.unique(y)):\n","        plt.scatter(x=X[y == cl, 0], \n","                    y=X[y == cl, 1],\n","                    alpha=0.6, \n","                    color=cmap(idx),\n","                    edgecolor='black',\n","                    marker=markers[idx], \n","                    label=cl)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"wCY8c_-M_-tH","colab_type":"text"},"source":["Training logistic regression classifier using the first 2 principal components."]},{"cell_type":"code","metadata":{"id":"vlrmt1iO_-tI","colab_type":"code","colab":{}},"source":["from sklearn.linear_model import LogisticRegression\n","\n","pca = PCA(n_components=2)\n","X_train_pca = pca.fit_transform(X_train_std)\n","X_test_pca = pca.transform(X_test_std)\n","\n","lr = LogisticRegression(multi_class='ovr', random_state=1, solver='lbfgs')\n","lr = lr.fit(X_train_pca, y_train)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"UDPEzXQX_-tM","colab_type":"code","colab":{}},"source":["plot_decision_regions(X_train_pca, y_train, classifier=lr)\n","plt.xlabel('PC 1')\n","plt.ylabel('PC 2')\n","plt.legend(loc='lower left')\n","plt.tight_layout()\n","# plt.savefig('images/05_04.png', dpi=300)\n","plt.show()"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"78RO3L_-_-tR","colab_type":"code","colab":{}},"source":["plot_decision_regions(X_test_pca, y_test, classifier=lr)\n","plt.xlabel('PC 1')\n","plt.ylabel('PC 2')\n","plt.legend(loc='lower left')\n","plt.tight_layout()\n","# plt.savefig('images/05_05.png', dpi=300)\n","plt.show()"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"Gr3lv13K_-tZ","colab_type":"code","colab":{}},"source":["pca = PCA(n_components=None)\n","X_train_pca = pca.fit_transform(X_train_std)\n","pca.explained_variance_ratio_"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"PKNPSZtX_-te","colab_type":"text"},"source":["<br>\n","<br>"]},{"cell_type":"markdown","metadata":{"id":"E9aSFLXX_-tf","colab_type":"text"},"source":["# Supervised data compression via linear discriminant analysis"]},{"cell_type":"markdown","metadata":{"id":"GpmT1Tym_-th","colab_type":"text"},"source":["## Principal component analysis versus linear discriminant analysis"]},{"cell_type":"code","metadata":{"id":"3yYNQXTS_-tj","colab_type":"code","colab":{}},"source":["Image(filename='images/05_06.png', width=400) "],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"2G6BFejB_-to","colab_type":"text"},"source":["## The inner workings of linear discriminant analysis"]},{"cell_type":"markdown","metadata":{"id":"mbw0Fv44_-tp","colab_type":"text"},"source":["<br>\n","<br>"]},{"cell_type":"markdown","metadata":{"id":"vf_vIAbv_-tq","colab_type":"text"},"source":["## Computing the scatter matrices"]},{"cell_type":"markdown","metadata":{"id":"Sb8qRVET_-tr","colab_type":"text"},"source":["Calculate the mean vectors for each class:"]},{"cell_type":"code","metadata":{"id":"ATfSpDu3_-ts","colab_type":"code","colab":{}},"source":["np.set_printoptions(precision=4)\n","\n","mean_vecs = []\n","for label in range(1, 4):\n","    mean_vecs.append(np.mean(X_train_std[y_train == label], axis=0))\n","    print('MV %s: %s\\n' % (label, mean_vecs[label - 1]))"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"JRdur4jS_-tw","colab_type":"text"},"source":["Compute the within-class scatter matrix:"]},{"cell_type":"code","metadata":{"id":"Rd9FsfVZ_-ty","colab_type":"code","colab":{}},"source":["d = 13 # number of features\n","S_W = np.zeros((d, d))\n","for label, mv in zip(range(1, 4), mean_vecs):\n","    class_scatter = np.zeros((d, d))  # scatter matrix for each class\n","    for row in X_train_std[y_train == label]:\n","        row, mv = row.reshape(d, 1), mv.reshape(d, 1)  # make column vectors\n","        class_scatter += (row - mv).dot((row - mv).T)\n","    S_W += class_scatter                          # sum class scatter matrices\n","\n","print('Within-class scatter matrix: %sx%s' % (S_W.shape[0], S_W.shape[1]))"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"FbQR6EHo_-t3","colab_type":"text"},"source":["Better: covariance matrix since classes are not equally distributed:"]},{"cell_type":"code","metadata":{"id":"rlS9p_dN_-t4","colab_type":"code","colab":{}},"source":["print('Class label distribution: %s' \n","      % np.bincount(y_train)[1:])"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"sHqz7pbF_-t9","colab_type":"code","colab":{}},"source":["d = 13  # number of features\n","S_W = np.zeros((d, d))\n","for label, mv in zip(range(1, 4), mean_vecs):\n","    class_scatter = np.cov(X_train_std[y_train == label].T)\n","    S_W += class_scatter\n","print('Scaled within-class scatter matrix: %sx%s' % (S_W.shape[0],\n","                                                     S_W.shape[1]))"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"CwZZxYdX_-uB","colab_type":"text"},"source":["Compute the between-class scatter matrix:"]},{"cell_type":"code","metadata":{"id":"BjHS7n1l_-uC","colab_type":"code","colab":{}},"source":["mean_overall = np.mean(X_train_std, axis=0)\n","d = 13  # number of features\n","S_B = np.zeros((d, d))\n","for i, mean_vec in enumerate(mean_vecs):\n","    n = X_train_std[y_train == i + 1, :].shape[0]\n","    mean_vec = mean_vec.reshape(d, 1)  # make column vector\n","    mean_overall = mean_overall.reshape(d, 1)  # make column vector\n","    S_B += n * (mean_vec - mean_overall).dot((mean_vec - mean_overall).T)\n","\n","print('Between-class scatter matrix: %sx%s' % (S_B.shape[0], S_B.shape[1]))"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"pfutwWkS_-uH","colab_type":"text"},"source":["<br>\n","<br>"]},{"cell_type":"markdown","metadata":{"id":"lqSlX8TV_-uI","colab_type":"text"},"source":["## Selecting linear discriminants for the new feature subspace"]},{"cell_type":"markdown","metadata":{"id":"y6ekwHJ9_-uK","colab_type":"text"},"source":["Solve the generalized eigenvalue problem for the matrix $S_W^{-1}S_B$:"]},{"cell_type":"code","metadata":{"id":"QeJ0hRET_-uL","colab_type":"code","colab":{}},"source":["eigen_vals, eigen_vecs = np.linalg.eig(np.linalg.inv(S_W).dot(S_B))"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"SimPE5UO_-uP","colab_type":"text"},"source":["**Note**:\n","    \n","Above, I used the [`numpy.linalg.eig`](http://docs.scipy.org/doc/numpy/reference/generated/numpy.linalg.eig.html) function to decompose the symmetric covariance matrix into its eigenvalues and eigenvectors.\n","    <pre>>>> eigen_vals, eigen_vecs = np.linalg.eig(cov_mat)</pre>\n","    This is not really a \"mistake,\" but probably suboptimal. It would be better to use [`numpy.linalg.eigh`](http://docs.scipy.org/doc/numpy/reference/generated/numpy.linalg.eigh.html) in such cases, which has been designed for [Hermetian matrices](https://en.wikipedia.org/wiki/Hermitian_matrix). The latter always returns real  eigenvalues; whereas the numerically less stable `np.linalg.eig` can decompose nonsymmetric square matrices, you may find that it returns complex eigenvalues in certain cases. (S.R.)\n"]},{"cell_type":"markdown","metadata":{"id":"DYRUZ0-0_-uS","colab_type":"text"},"source":["Sort eigenvectors in descending order of the eigenvalues:"]},{"cell_type":"code","metadata":{"id":"sfhd7Jgy_-uT","colab_type":"code","colab":{}},"source":["# Make a list of (eigenvalue, eigenvector) tuples\n","eigen_pairs = [(np.abs(eigen_vals[i]), eigen_vecs[:, i])\n","               for i in range(len(eigen_vals))]\n","\n","# Sort the (eigenvalue, eigenvector) tuples from high to low\n","eigen_pairs = sorted(eigen_pairs, key=lambda k: k[0], reverse=True)\n","\n","# Visually confirm that the list is correctly sorted by decreasing eigenvalues\n","\n","print('Eigenvalues in descending order:\\n')\n","for eigen_val in eigen_pairs:\n","    print(eigen_val[0])"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"4xPVLaCv_-uW","colab_type":"code","colab":{}},"source":["tot = sum(eigen_vals.real)\n","discr = [(i / tot) for i in sorted(eigen_vals.real, reverse=True)]\n","cum_discr = np.cumsum(discr)\n","\n","plt.bar(range(1, 14), discr, alpha=0.5, align='center',\n","        label='Individual \"discriminability\"')\n","plt.step(range(1, 14), cum_discr, where='mid',\n","         label='Cumulative \"discriminability\"')\n","plt.ylabel('\"Discriminability\" ratio')\n","plt.xlabel('Linear discriminants')\n","plt.ylim([-0.1, 1.1])\n","plt.legend(loc='best')\n","plt.tight_layout()\n","# plt.savefig('images/05_07.png', dpi=300)\n","plt.show()"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"6lZ6Gi5T_-ub","colab_type":"code","colab":{}},"source":["w = np.hstack((eigen_pairs[0][1][:, np.newaxis].real,\n","              eigen_pairs[1][1][:, np.newaxis].real))\n","print('Matrix W:\\n', w)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"i8SufAJl_-uf","colab_type":"text"},"source":["<br>\n","<br>"]},{"cell_type":"markdown","metadata":{"id":"ZpqWfYcG_-ug","colab_type":"text"},"source":["## Projecting examples onto the new feature space"]},{"cell_type":"code","metadata":{"id":"5Wi8Zviq_-ug","colab_type":"code","colab":{}},"source":["X_train_lda = X_train_std.dot(w)\n","colors = ['r', 'b', 'g']\n","markers = ['s', 'x', 'o']\n","\n","for l, c, m in zip(np.unique(y_train), colors, markers):\n","    plt.scatter(X_train_lda[y_train == l, 0],\n","                X_train_lda[y_train == l, 1] * (-1),\n","                c=c, label=l, marker=m)\n","\n","plt.xlabel('LD 1')\n","plt.ylabel('LD 2')\n","plt.legend(loc='lower right')\n","plt.tight_layout()\n","# plt.savefig('images/05_08.png', dpi=300)\n","plt.show()"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"aplLRwlF_-uj","colab_type":"text"},"source":["<br>\n","<br>"]},{"cell_type":"markdown","metadata":{"id":"K90j1xME_-uk","colab_type":"text"},"source":["## LDA via scikit-learn"]},{"cell_type":"code","metadata":{"id":"CgH4wXOK_-ul","colab_type":"code","colab":{}},"source":["from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA\n","\n","lda = LDA(n_components=2)\n","X_train_lda = lda.fit_transform(X_train_std, y_train)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"EWSBQc20_-uo","colab_type":"code","colab":{}},"source":["from sklearn.linear_model import LogisticRegression\n","\n","lr = LogisticRegression(multi_class='ovr', random_state=1, solver='lbfgs')\n","lr = lr.fit(X_train_lda, y_train)\n","\n","plot_decision_regions(X_train_lda, y_train, classifier=lr)\n","plt.xlabel('LD 1')\n","plt.ylabel('LD 2')\n","plt.legend(loc='lower left')\n","plt.tight_layout()\n","# plt.savefig('images/05_09.png', dpi=300)\n","plt.show()"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"63FjbHiv_-us","colab_type":"code","colab":{}},"source":["X_test_lda = lda.transform(X_test_std)\n","\n","plot_decision_regions(X_test_lda, y_test, classifier=lr)\n","plt.xlabel('LD 1')\n","plt.ylabel('LD 2')\n","plt.legend(loc='lower left')\n","plt.tight_layout()\n","# plt.savefig('images/05_10.png', dpi=300)\n","plt.show()"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"wp5CBWe5_-uv","colab_type":"text"},"source":["<br>\n","<br>"]},{"cell_type":"markdown","metadata":{"id":"jgK6rteq_-uw","colab_type":"text"},"source":["# Using kernel principal component analysis for nonlinear mappings"]},{"cell_type":"code","metadata":{"id":"oEqL65fk_-ux","colab_type":"code","colab":{}},"source":["Image(filename='images/05_11.png', width=500) "],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"pUyvdLFh_-u0","colab_type":"text"},"source":["<br>\n","<br>"]},{"cell_type":"markdown","metadata":{"id":"vjSXuiDS_-u1","colab_type":"text"},"source":["## Implementing a kernel principal component analysis in Python"]},{"cell_type":"code","metadata":{"id":"s3iA_1Xk_-u2","colab_type":"code","colab":{}},"source":["from scipy.spatial.distance import pdist, squareform\n","from scipy import exp\n","from scipy.linalg import eigh\n","import numpy as np\n","\n","def rbf_kernel_pca(X, gamma, n_components):\n","    \"\"\"\n","    RBF kernel PCA implementation.\n","\n","    Parameters\n","    ------------\n","    X: {NumPy ndarray}, shape = [n_examples, n_features]\n","        \n","    gamma: float\n","      Tuning parameter of the RBF kernel\n","        \n","    n_components: int\n","      Number of principal components to return\n","\n","    Returns\n","    ------------\n","     X_pc: {NumPy ndarray}, shape = [n_examples, k_features]\n","       Projected dataset   \n","\n","    \"\"\"\n","    # Calculate pairwise squared Euclidean distances\n","    # in the MxN dimensional dataset.\n","    sq_dists = pdist(X, 'sqeuclidean')\n","\n","    # Convert pairwise distances into a square matrix.\n","    mat_sq_dists = squareform(sq_dists)\n","\n","    # Compute the symmetric kernel matrix.\n","    K = exp(-gamma * mat_sq_dists)\n","\n","    # Center the kernel matrix.\n","    N = K.shape[0]\n","    one_n = np.ones((N, N)) / N\n","    K = K - one_n.dot(K) - K.dot(one_n) + one_n.dot(K).dot(one_n)\n","\n","    # Obtaining eigenpairs from the centered kernel matrix\n","    # scipy.linalg.eigh returns them in ascending order\n","    eigvals, eigvecs = eigh(K)\n","    eigvals, eigvecs = eigvals[::-1], eigvecs[:, ::-1]\n","\n","    # Collect the top k eigenvectors (projected examples)\n","    X_pc = np.column_stack([eigvecs[:, i]\n","                            for i in range(n_components)])\n","\n","    return X_pc"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"BLuL9TxN_-u6","colab_type":"text"},"source":["<br>"]},{"cell_type":"markdown","metadata":{"id":"3DvFl0uf_-u7","colab_type":"text"},"source":["### Example 1: Separating half-moon shapes"]},{"cell_type":"code","metadata":{"id":"0lIbql8J_-u8","colab_type":"code","colab":{}},"source":["import matplotlib.pyplot as plt\n","from sklearn.datasets import make_moons\n","\n","X, y = make_moons(n_samples=100, random_state=123)\n","\n","plt.scatter(X[y == 0, 0], X[y == 0, 1], color='red', marker='^', alpha=0.5)\n","plt.scatter(X[y == 1, 0], X[y == 1, 1], color='blue', marker='o', alpha=0.5)\n","\n","plt.tight_layout()\n","# plt.savefig('images/05_12.png', dpi=300)\n","plt.show()"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"qEZHvphU_-u-","colab_type":"code","colab":{}},"source":["from sklearn.decomposition import PCA\n","\n","scikit_pca = PCA(n_components=2)\n","X_spca = scikit_pca.fit_transform(X)\n","\n","fig, ax = plt.subplots(nrows=1, ncols=2, figsize=(7, 3))\n","\n","ax[0].scatter(X_spca[y == 0, 0], X_spca[y == 0, 1],\n","              color='red', marker='^', alpha=0.5)\n","ax[0].scatter(X_spca[y == 1, 0], X_spca[y == 1, 1],\n","              color='blue', marker='o', alpha=0.5)\n","\n","ax[1].scatter(X_spca[y == 0, 0], np.zeros((50, 1)) + 0.02,\n","              color='red', marker='^', alpha=0.5)\n","ax[1].scatter(X_spca[y == 1, 0], np.zeros((50, 1)) - 0.02,\n","              color='blue', marker='o', alpha=0.5)\n","\n","ax[0].set_xlabel('PC1')\n","ax[0].set_ylabel('PC2')\n","ax[1].set_ylim([-1, 1])\n","ax[1].set_yticks([])\n","ax[1].set_xlabel('PC1')\n","\n","plt.tight_layout()\n","# plt.savefig('images/05_13.png', dpi=300)\n","plt.show()"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"5Y_vudDI_-vB","colab_type":"code","colab":{}},"source":["X_kpca = rbf_kernel_pca(X, gamma=15, n_components=2)\n","\n","fig, ax = plt.subplots(nrows=1, ncols=2, figsize=(7, 3))\n","ax[0].scatter(X_kpca[y==0, 0], X_kpca[y==0, 1], \n","            color='red', marker='^', alpha=0.5)\n","ax[0].scatter(X_kpca[y==1, 0], X_kpca[y==1, 1],\n","            color='blue', marker='o', alpha=0.5)\n","\n","ax[1].scatter(X_kpca[y==0, 0], np.zeros((50, 1))+0.02, \n","            color='red', marker='^', alpha=0.5)\n","ax[1].scatter(X_kpca[y==1, 0], np.zeros((50, 1))-0.02,\n","            color='blue', marker='o', alpha=0.5)\n","\n","ax[0].set_xlabel('PC1')\n","ax[0].set_ylabel('PC2')\n","ax[1].set_ylim([-1, 1])\n","ax[1].set_yticks([])\n","ax[1].set_xlabel('PC1')\n","\n","plt.tight_layout()\n","# plt.savefig('images/05_14.png', dpi=300)\n","plt.show()"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"eMAABzrb_-vD","colab_type":"text"},"source":["<br>"]},{"cell_type":"markdown","metadata":{"id":"IsksXjXN_-vE","colab_type":"text"},"source":["### Example 2: Separating concentric circles"]},{"cell_type":"code","metadata":{"id":"KEV4bJwg_-vF","colab_type":"code","colab":{}},"source":["from sklearn.datasets import make_circles\n","\n","X, y = make_circles(n_samples=1000, random_state=123, noise=0.1, factor=0.2)\n","\n","plt.scatter(X[y == 0, 0], X[y == 0, 1], color='red', marker='^', alpha=0.5)\n","plt.scatter(X[y == 1, 0], X[y == 1, 1], color='blue', marker='o', alpha=0.5)\n","\n","plt.tight_layout()\n","# plt.savefig('images/05_15.png', dpi=300)\n","plt.show()"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"7ABELH0N_-vH","colab_type":"code","colab":{}},"source":["scikit_pca = PCA(n_components=2)\n","X_spca = scikit_pca.fit_transform(X)\n","\n","fig, ax = plt.subplots(nrows=1, ncols=2, figsize=(7, 3))\n","\n","ax[0].scatter(X_spca[y == 0, 0], X_spca[y == 0, 1],\n","              color='red', marker='^', alpha=0.5)\n","ax[0].scatter(X_spca[y == 1, 0], X_spca[y == 1, 1],\n","              color='blue', marker='o', alpha=0.5)\n","\n","ax[1].scatter(X_spca[y == 0, 0], np.zeros((500, 1)) + 0.02,\n","              color='red', marker='^', alpha=0.5)\n","ax[1].scatter(X_spca[y == 1, 0], np.zeros((500, 1)) - 0.02,\n","              color='blue', marker='o', alpha=0.5)\n","\n","ax[0].set_xlabel('PC1')\n","ax[0].set_ylabel('PC2')\n","ax[1].set_ylim([-1, 1])\n","ax[1].set_yticks([])\n","ax[1].set_xlabel('PC1')\n","\n","plt.tight_layout()\n","# plt.savefig('images/05_16.png', dpi=300)\n","plt.show()"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"lUHIYiLQ_-vK","colab_type":"code","colab":{}},"source":["X_kpca = rbf_kernel_pca(X, gamma=15, n_components=2)\n","\n","fig, ax = plt.subplots(nrows=1, ncols=2, figsize=(7, 3))\n","ax[0].scatter(X_kpca[y == 0, 0], X_kpca[y == 0, 1],\n","              color='red', marker='^', alpha=0.5)\n","ax[0].scatter(X_kpca[y == 1, 0], X_kpca[y == 1, 1],\n","              color='blue', marker='o', alpha=0.5)\n","\n","ax[1].scatter(X_kpca[y == 0, 0], np.zeros((500, 1)) + 0.02,\n","              color='red', marker='^', alpha=0.5)\n","ax[1].scatter(X_kpca[y == 1, 0], np.zeros((500, 1)) - 0.02,\n","              color='blue', marker='o', alpha=0.5)\n","\n","ax[0].set_xlabel('PC1')\n","ax[0].set_ylabel('PC2')\n","ax[1].set_ylim([-1, 1])\n","ax[1].set_yticks([])\n","ax[1].set_xlabel('PC1')\n","\n","plt.tight_layout()\n","# plt.savefig('images/05_17.png', dpi=300)\n","plt.show()"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"RvP2PAdl_-vN","colab_type":"text"},"source":["<br>\n","<br>"]},{"cell_type":"markdown","metadata":{"id":"gJSnprgU_-vQ","colab_type":"text"},"source":["## Projecting new data points"]},{"cell_type":"code","metadata":{"id":"JjbaGUJP_-vR","colab_type":"code","colab":{}},"source":["from scipy.spatial.distance import pdist, squareform\n","from scipy import exp\n","from scipy.linalg import eigh\n","import numpy as np\n","\n","def rbf_kernel_pca(X, gamma, n_components):\n","    \"\"\"\n","    RBF kernel PCA implementation.\n","\n","    Parameters\n","    ------------\n","    X: {NumPy ndarray}, shape = [n_examples, n_features]\n","        \n","    gamma: float\n","      Tuning parameter of the RBF kernel\n","        \n","    n_components: int\n","      Number of principal components to return\n","\n","    Returns\n","    ------------\n","     alphas: {NumPy ndarray}, shape = [n_examples, k_features]\n","       Projected dataset \n","     \n","     lambdas: list\n","       Eigenvalues\n","\n","    \"\"\"\n","    # Calculate pairwise squared Euclidean distances\n","    # in the MxN dimensional dataset.\n","    sq_dists = pdist(X, 'sqeuclidean')\n","\n","    # Convert pairwise distances into a square matrix.\n","    mat_sq_dists = squareform(sq_dists)\n","\n","    # Compute the symmetric kernel matrix.\n","    K = exp(-gamma * mat_sq_dists)\n","\n","    # Center the kernel matrix.\n","    N = K.shape[0]\n","    one_n = np.ones((N, N)) / N\n","    K = K - one_n.dot(K) - K.dot(one_n) + one_n.dot(K).dot(one_n)\n","\n","    # Obtaining eigenpairs from the centered kernel matrix\n","    # scipy.linalg.eigh returns them in ascending order\n","    eigvals, eigvecs = eigh(K)\n","    eigvals, eigvecs = eigvals[::-1], eigvecs[:, ::-1]\n","\n","    # Collect the top k eigenvectors (projected examples)\n","    alphas = np.column_stack([eigvecs[:, i]\n","                              for i in range(n_components)])\n","\n","    # Collect the corresponding eigenvalues\n","    lambdas = [eigvals[i] for i in range(n_components)]\n","\n","    return alphas, lambdas"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"u4t-Cxd9_-vU","colab_type":"code","colab":{}},"source":["X, y = make_moons(n_samples=100, random_state=123)\n","alphas, lambdas = rbf_kernel_pca(X, gamma=15, n_components=1)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"5oFmFFAC_-vX","colab_type":"code","colab":{}},"source":["x_new = X[25]\n","x_new"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"kS2GbhEw_-va","colab_type":"code","colab":{}},"source":["x_proj = alphas[25] # original projection\n","x_proj"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"ptSH7B9H_-ve","colab_type":"code","colab":{}},"source":["def project_x(x_new, X, gamma, alphas, lambdas):\n","    pair_dist = np.array([np.sum((x_new - row)**2) for row in X])\n","    k = np.exp(-gamma * pair_dist)\n","    return k.dot(alphas / lambdas)\n","\n","# projection of the \"new\" datapoint\n","x_reproj = project_x(x_new, X, gamma=15, alphas=alphas, lambdas=lambdas)\n","x_reproj "],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"KOcXP4ul_-vk","colab_type":"code","colab":{}},"source":["plt.scatter(alphas[y == 0, 0], np.zeros((50)),\n","            color='red', marker='^', alpha=0.5)\n","plt.scatter(alphas[y == 1, 0], np.zeros((50)),\n","            color='blue', marker='o', alpha=0.5)\n","plt.scatter(x_proj, 0, color='black',\n","            label='Original projection of point X[25]', marker='x', s=100)\n","plt.scatter(x_reproj, 0, color='green',\n","            label='Remapped point X[25]', marker='x', s=500)\n","plt.yticks([], [])\n","plt.legend(scatterpoints=1)\n","\n","plt.tight_layout()\n","# plt.savefig('images/05_18.png', dpi=300)\n","plt.show()"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"zeZfuAqV_-vn","colab_type":"text"},"source":["<br>\n","<br>"]},{"cell_type":"markdown","metadata":{"id":"qZD8SDKP_-vn","colab_type":"text"},"source":["## Kernel principal component analysis in scikit-learn"]},{"cell_type":"code","metadata":{"id":"BJ8UzlDd_-vo","colab_type":"code","colab":{}},"source":["from sklearn.decomposition import KernelPCA\n","\n","X, y = make_moons(n_samples=100, random_state=123)\n","scikit_kpca = KernelPCA(n_components=2, kernel='rbf', gamma=15)\n","X_skernpca = scikit_kpca.fit_transform(X)\n","\n","plt.scatter(X_skernpca[y == 0, 0], X_skernpca[y == 0, 1],\n","            color='red', marker='^', alpha=0.5)\n","plt.scatter(X_skernpca[y == 1, 0], X_skernpca[y == 1, 1],\n","            color='blue', marker='o', alpha=0.5)\n","\n","plt.xlabel('PC1')\n","plt.ylabel('PC2')\n","plt.tight_layout()\n","# plt.savefig('images/05_19.png', dpi=300)\n","plt.show()"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"hxOEVizm_-vq","colab_type":"text"},"source":["<br>\n","<br>"]},{"cell_type":"markdown","metadata":{"id":"NEMT_EBN_-vr","colab_type":"text"},"source":["# Summary"]},{"cell_type":"markdown","metadata":{"id":"xc9C8w5Y_-vs","colab_type":"text"},"source":["..."]},{"cell_type":"markdown","metadata":{"id":"tkQ6MHa3_-vs","colab_type":"text"},"source":["---\n","\n","Readers may ignore the next cell."]},{"cell_type":"code","metadata":{"id":"KPGL9i2N_-vt","colab_type":"code","colab":{}},"source":["! python ../.convert_notebook_to_script.py --input ch05.ipynb --output ch05.py"],"execution_count":0,"outputs":[]}]}